{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE474/574 - Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Sentiment Analysis\n",
    "\n",
    "In the code provided below, you need to add code wherever specified by `TODO:`. \n",
    "\n",
    "> You will be using a Python collection class - `Counter` to maintain the word counts. \n",
    "\n",
    "> See https://docs.python.org/2/library/collections.html for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# read data files \n",
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews_all = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "sentiments_all = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()\n",
    "\n",
    "# load vocabulary\n",
    "g = open('vocab.txt','r')\n",
    "vocab = [s.strip() for s in g.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a set of 25000 movie reviews, along with a `POSITIVE` or `NEGATIVE` sentiment label assigned to the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A POSITIVE review:\n",
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "\n",
      "A NEGATIVE review:\n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n"
     ]
    }
   ],
   "source": [
    "# Check out sample reviews\n",
    "print('A {} review:'.format(sentiments_all[0]))\n",
    "print(reviews_all[0])\n",
    "print('\\nA {} review:'.format(sentiments_all[1]))\n",
    "print(reviews_all[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# split into training and test data\n",
    "reviews_train,reviews_test = reviews_all[0:24000],reviews_all[24000:]\n",
    "sentiments_train,sentiments_test = sentiments_all[0:24000],sentiments_all[24000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# maintain Counter objects to store positive, negative and total counts for\n",
    "# all the words present in the positive, negative and total reviews.\n",
    "positive_word_count = Counter()\n",
    "negative_word_count = Counter()\n",
    "total_counts = Counter()\n",
    "# TODO: Loop over all the words in the vocabulary\n",
    "# and increment the counts in the appropriate counter objects\n",
    "# based on the training data\n",
    "\n",
    "\n",
    "for i in range(0, len(sentiments_train)):\n",
    "    if sentiments_train[i] == 'NEGATIVE':\n",
    "        negative_word_count.update(reviews_train[i].split())\n",
    "    else:\n",
    "        positive_word_count.update(reviews_train[i].split())\n",
    "        \n",
    "total_counts.update(positive_word_count)\n",
    "total_counts.update(negative_word_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos-to-neg ratio for 'the' = 1.0618582280413789\n"
     ]
    }
   ],
   "source": [
    "# maintain a Counter object to store positive to negative ratios \n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "# Calculate the ratios of positive and negative uses of the most common words\n",
    "# Consider words to be \"common\" if they've been used at least 100 times\n",
    "for term,cnt in list(total_counts.most_common()):\n",
    "    if(cnt > 100):\n",
    "        # TODO: Code for calculating the ratios (remove the next line)\n",
    "        if(negative_word_count[term] != 0):\n",
    "            ratio = positive_word_count[term] / negative_word_count[term]\n",
    "        else:\n",
    "            ratio = positive_word_count[term]\n",
    "        pos_neg_ratios.update({term: ratio})\n",
    "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos-to-neg ratio for 'the' = 1.0618582280413789\n",
      "Pos-to-neg ratio for 'amazing' = 4.031496062992126\n",
      "Pos-to-neg ratio for 'terrible' = 0.17256637168141592\n"
     ]
    }
   ],
   "source": [
    "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
    "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a log of the ratio\n",
    "for word,ratio in pos_neg_ratios.most_common():\n",
    "    pos_neg_ratios[word] = np.log(ratio)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASQUlEQVR4nO3df6zdd33f8edrdhNGO+qAbyi1rdld3a4pZSK6DdnQNoZLyC/F+aORkq3FopGsdaGDUQROkRapFVJYp6ZFZak84uFoUdKI0sUq6VI30KFKTchNgIBjaK5CFl8S8EUOaTdUmMt7f5yPmxP72vfHufec2J/nQ7q63+/7+zn3vL9K9Doff873nG+qCklSH/7epBuQJI2PoS9JHTH0Jakjhr4kdcTQl6SOrJ90A2eycePG2rp166TbkKSzyqOPPvqtqppa6NjLOvS3bt3KzMzMpNuQpLNKkv99umMu70hSRwx9SeqIoS9JHTH0Jakjhr4kdWTR0E+yL8nRJF8+qf4rSb6a5FCS/zRUvznJbDv29qH65a02m2TP6p6GJGkplnLJ5seB3wXuPFFI8q+AncAbquq7SS5s9YuA64GfBn4U+NMkP9Ee9lHgbcAc8EiSA1X1xGqdiCRpcYuGflV9NsnWk8q/DNxaVd9tY462+k7gnlb/WpJZ4JJ2bLaqngJIck8ba+hL0hitdE3/J4B/nuThJP8ryc+2+ibgyNC4uVY7Xf0USXYnmUkyMz8/v8L2JEkLWeknctcDFwCXAj8L3Jvkx4AsMLZY+MVlwbu3VNVeYC/A9PS0d3jRy9bWPZ9a8WOfvvWqVexEWrqVhv4c8Mka3Hbrc0m+D2xs9S1D4zYDz7bt09UlSWOy0uWd/wG8FaC9UXse8C3gAHB9kvOTbAO2A58DHgG2J9mW5DwGb/YeGLV5SdLyLDrTT3I38BZgY5I54BZgH7CvXcb5PWBXm/UfSnIvgzdojwM3VdXftr/zLuABYB2wr6oOrcH5SJLOYClX79xwmkO/cJrxHwI+tED9fuD+ZXUnSVpVfiJXkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOrJo6CfZl+RouzXiycfel6SSbGz7SfKRJLNJHk9y8dDYXUmebD+7Vvc0JElLsZSZ/seBy08uJtkCvA14Zqh8BYOboW8HdgO3t7GvZnBv3TcBlwC3JLlglMYlScu3aOhX1WeBYwscug14P1BDtZ3AnTXwELAhyeuAtwMHq+pYVT0PHGSBFxJJ0tpa0Zp+kmuAr1fVF086tAk4MrQ/12qnqy/0t3cnmUkyMz8/v5L2JEmnsezQT/JK4IPAf1zo8AK1OkP91GLV3qqarqrpqamp5bYnSTqDlcz0/xGwDfhikqeBzcBjSX6EwQx+y9DYzcCzZ6hLksZo2aFfVV+qqguramtVbWUQ6BdX1TeAA8A72lU8lwIvVNVzwAPAZUkuaG/gXtZqkqQxWsolm3cDfwH8ZJK5JDeeYfj9wFPALPBfgX8HUFXHgN8AHmk/v95qkqQxWr/YgKq6YZHjW4e2C7jpNOP2AfuW2Z8kaRX5iVxJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyFJul7gvydEkXx6q/WaSryR5PMkfJtkwdOzmJLNJvprk7UP1y1ttNsme1T8VSdJiljLT/zhw+Um1g8Drq+oNwF8CNwMkuQi4Hvjp9pj/kmRdknXAR4ErgIuAG9pYSdIYLRr6VfVZ4NhJtT+pquNt9yFgc9veCdxTVd+tqq8xuEH6Je1ntqqeqqrvAfe0sZKkMVqNNf1fAv64bW8Cjgwdm2u109VPkWR3kpkkM/Pz86vQniTphJFCP8kHgePAXSdKCwyrM9RPLVbtrarpqpqempoapT1J0knWr/SBSXYBVwM7qupEgM8BW4aGbQaebdunq0uSxmRFM/0klwMfAK6pqu8MHToAXJ/k/CTbgO3A54BHgO1JtiU5j8GbvQdGa12StFyLzvST3A28BdiYZA64hcHVOucDB5MAPFRV/7aqDiW5F3iCwbLPTVX1t+3vvAt4AFgH7KuqQ2twPpKkM1g09KvqhgXKd5xh/IeADy1Qvx+4f1ndSZJWlZ/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4sGvpJ9iU5muTLQ7VXJzmY5Mn2+4JWT5KPJJlN8niSi4ces6uNf7LdVF2SNGZLmel/HLj8pNoe4MGq2g482PYBrmBwM/TtwG7gdhi8SDC4t+6bgEuAW068UEiSxmfR0K+qzwLHTirvBPa37f3AtUP1O2vgIWBDktcBbwcOVtWxqnoeOMipLySSpDW20jX911bVcwDt94Wtvgk4MjRurtVOVz9Fkt1JZpLMzM/Pr7A9SdJCVvuN3CxQqzPUTy1W7a2q6aqanpqaWtXmJKl3Kw39b7ZlG9rvo60+B2wZGrcZePYMdUnSGK009A8AJ67A2QXcN1R/R7uK51Lghbb88wBwWZIL2hu4l7WaJGmM1i82IMndwFuAjUnmGFyFcytwb5IbgWeA69rw+4ErgVngO8A7AarqWJLfAB5p4369qk5+c1iStMYWDf2quuE0h3YsMLaAm07zd/YB+5bVnSRpVfmJXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOrLodfqSVt/WPZ9a8WOfvvWqVexEvXGmL0kdMfQlqSOGviR1xNCXpI4Y+pLUEa/eUddGuYpGOhs505ekjhj6ktQRQ1+SOjJS6Cf5D0kOJflykruTvCLJtiQPJ3kyye8nOa+NPb/tz7bjW1fjBCRJS7fi0E+yCfj3wHRVvR5YB1wPfBi4raq2A88DN7aH3Ag8X1U/DtzWxkmSxmjU5Z31wN9Psh54JfAc8FbgE+34fuDatr2z7dOO70iSEZ9fkrQMKw79qvo68J+BZxiE/QvAo8C3q+p4GzYHbGrbm4Aj7bHH2/jXnPx3k+xOMpNkZn5+fqXtSZIWMMryzgUMZu/bgB8FfhC4YoGhdeIhZzj2YqFqb1VNV9X01NTUStuTJC1glOWdnwO+VlXzVfX/gE8C/wzY0JZ7ADYDz7btOWALQDv+w8CxEZ5fkrRMo4T+M8ClSV7Z1uZ3AE8AnwF+vo3ZBdzXtg+0fdrxT1fVKTN9SdLaGWVN/2EGb8g+Bnyp/a29wAeA9yaZZbBmf0d7yB3Aa1r9vcCeEfqWJK3ASN+9U1W3ALecVH4KuGSBsX8DXDfK80mSRuMnciWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjI4V+kg1JPpHkK0kOJ/mnSV6d5GCSJ9vvC9rYJPlIktkkjye5eHVOQZK0VKPO9H8H+J9V9Y+BfwIcZnDv2werajvwIC/eC/cKYHv72Q3cPuJzS5KWacWhn+RVwL+g3fi8qr5XVd8GdgL727D9wLVteydwZw08BGxI8roVdy5JWrZRZvo/BswD/y3J55N8LMkPAq+tqucA2u8L2/hNwJGhx8+12ksk2Z1kJsnM/Pz8CO1Jkk42SuivBy4Gbq+qNwL/lxeXchaSBWp1SqFqb1VNV9X01NTUCO1Jkk42SujPAXNV9XDb/wSDF4Fvnli2ab+PDo3fMvT4zcCzIzy/JGmZVhz6VfUN4EiSn2ylHcATwAFgV6vtAu5r2weAd7SreC4FXjixDCRJGo/1Iz7+V4C7kpwHPAW8k8ELyb1JbgSeAa5rY+8HrgRmge+0sZKkMRop9KvqC8D0Aod2LDC2gJtGeT5J0mj8RK4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZOTQT7IuyeeT/FHb35bk4SRPJvn9ditFkpzf9mfb8a2jPrckaXlWY6b/buDw0P6HgduqajvwPHBjq98IPF9VPw7c1sZJksZopNBPshm4CvhY2w/wVuATbch+4Nq2vbPt047vaOMlSWMy6kz/t4H3A99v+68Bvl1Vx9v+HLCpbW8CjgC04y+08S+RZHeSmSQz8/PzI7YnSRq24tBPcjVwtKoeHS4vMLSWcOzFQtXeqpququmpqamVtidJWsD6ER77ZuCaJFcCrwBexWDmvyHJ+jab3ww828bPAVuAuSTrgR8Gjo3w/JKkZVrxTL+qbq6qzVW1Fbge+HRV/RvgM8DPt2G7gPva9oG2Tzv+6ao6ZaYvSVo7a3Gd/geA9yaZZbBmf0er3wG8ptXfC+xZg+eWJJ3BKMs7f6eq/gz4s7b9FHDJAmP+BrhuNZ5PkrQyfiJXkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyKp89440KVv3fGrSLUhnFWf6ktQRQ1+SOuLyjnSWGXVJ6+lbr1qlTnQ2cqYvSR0x9CWpIysO/SRbknwmyeEkh5K8u9VfneRgkifb7wtaPUk+kmQ2yeNJLl6tk5AkLc0oM/3jwK9W1U8BlwI3JbmIwb1vH6yq7cCDvHgv3CuA7e1nN3D7CM8tSVqBFYd+VT1XVY+17b8GDgObgJ3A/jZsP3Bt294J3FkDDwEbkrxuxZ1LkpZtVdb0k2wF3gg8DLy2qp6DwQsDcGEbtgk4MvSwuVY7+W/tTjKTZGZ+fn412pMkNSOHfpIfAv4AeE9V/dWZhi5Qq1MKVXurarqqpqempkZtT5I0ZKTQT/IDDAL/rqr6ZCt/88SyTft9tNXngC1DD98MPDvK80uSlmeUq3cC3AEcrqrfGjp0ANjVtncB9w3V39Gu4rkUeOHEMpAkaTxG+UTum4FfBL6U5Aut9mvArcC9SW4EngGua8fuB64EZoHvAO8c4bklSSuw4tCvqj9n4XV6gB0LjC/gppU+nyRpdH4iV5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQR75GriRv1nq+Sls6ZviR1xNCXpI64vCN1ZpTltKdvvWoVO9EkONOXpI4Y+pLUEZd3tCq8Akc6OzjTl6SOGPqS1JGxL+8kuRz4HWAd8LGqunXcPUhaGa/8OfuNNfSTrAM+CrwNmAMeSXKgqp4YZx9amOvy0rlv3DP9S4DZqnoKIMk9wE7A0G8MXp2rJvX/tv/CeKlxh/4m4MjQ/hzwpuEBSXYDu9vu/0ny1TH1tpo2At+adBMT4rn36WV77vnwmj/Fy/Hc/+HpDow79LNArV6yU7UX2DuedtZGkpmqmp50H5PguXvuvTnbzn3cV+/MAVuG9jcDz465B0nq1rhD/xFge5JtSc4DrgcOjLkHSerWWJd3qup4kncBDzC4ZHNfVR0aZw9jclYvT43Ic++T536WSFUtPkqSdE7wE7mS1BFDX5I6YuivsSTvS1JJNk66l3FJ8ptJvpLk8SR/mGTDpHtaa0kuT/LVJLNJ9ky6n3FJsiXJZ5IcTnIoybsn3dO4JVmX5PNJ/mjSvSyFob+Gkmxh8JUTz0y6lzE7CLy+qt4A/CVw84T7WVNDXy9yBXARcEOSiybb1dgcB361qn4KuBS4qaNzP+HdwOFJN7FUhv7aug14Pyd9AO1cV1V/UlXH2+5DDD6PcS77u68XqarvASe+XuScV1XPVdVjbfuvGYTfpsl2NT5JNgNXAR+bdC9LZeivkSTXAF+vqi9OupcJ+yXgjyfdxBpb6OtFugm+E5JsBd4IPDzZTsbqtxlM7L4/6UaWyjtnjSDJnwI/ssChDwK/Blw23o7G50znXlX3tTEfZPDP/7vG2dsELPr1Iue6JD8E/AHwnqr6q0n3Mw5JrgaOVtWjSd4y6X6WytAfQVX93EL1JD8DbAO+mAQGyxuPJbmkqr4xxhbXzOnO/YQku4CrgR117n8YpOuvF0nyAwwC/66q+uSk+xmjNwPXJLkSeAXwqiT/vap+YcJ9nZEfzhqDJE8D01X1cvsmvjXRbpTzW8C/rKr5Sfez1pKsZ/CG9Q7g6wy+buRfn6OfNn+JDGY1+4FjVfWeSfczKW2m/76qunrSvSzGNX2thd8F/gFwMMkXkvzepBtaS+1N6xNfL3IYuLeHwG/eDPwi8Nb23/oLbearlyln+pLUEWf6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR15P8DYHDdDUSFQ1sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the distribution of the log-ratio scores\n",
    "scores = np.array(list(pos_neg_ratios.values()))\n",
    "vocab_selected = list(pos_neg_ratios.keys())\n",
    "\n",
    "h = plt.hist(scores,bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above histogram should give you an idea about the distribution of the scores.\n",
    "\n",
    "Notice how the scores are distributed around 0. A word with score 0 can be considered as `neutral`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realize\n",
      "hands\n",
      "extreme\n",
      "beat\n",
      "onto\n",
      "psycho\n",
      "test\n",
      "obsessed\n",
      "choose\n",
      "speech\n"
     ]
    }
   ],
   "source": [
    "# Print few words with neutral score\n",
    "for ind in np.where(scores == 0)[0][0:10]:\n",
    "    print(vocab_selected[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPROACH 1** Implement a simple non-machine learning that only uses the log-ratios to determine if a review is positive or negative. This function will be applied to the test data to calculate the accuracy of the model. \n",
    "\n",
    "_See the assignment document for hints._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def nonml_classifier(review,pos_neg_ratios):\n",
    "    \n",
    "    '''\n",
    "    Function that determines the sentiment for a given review.\n",
    "    \n",
    "    Inputs:\n",
    "      review - A text containing a movie review\n",
    "      pos_neg_ratios - A Counter object containing frequent words\n",
    "                       and corresponding log positive-negative ratio\n",
    "    Return:\n",
    "      sentiment - 'NEGATIVE' or 'POSITIVE'\n",
    "    '''\n",
    "    \n",
    "    count_reviews = Counter()\n",
    "    count_reviews.update(review.split())\n",
    "    \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    for i in count_reviews:\n",
    "        if i in pos_neg_ratios:\n",
    "            pos += pos_neg_ratios[i] \n",
    "        else:\n",
    "            neg += pos_neg_ratios[i]\n",
    "    \n",
    "    res = pos / neg\n",
    "    \n",
    "    return \"POSITIVE\" if res > 1 else \"NEGATIVE\"\n",
    "    \n",
    "    # TODO: Implement the algorithm here. Change the next line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model = 0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_default/lib/python3.7/site-packages/ipykernel/__main__.py:26: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "predictions_test = []\n",
    "for r in reviews_test:\n",
    "    l = nonml_classifier(r,pos_neg_ratios)\n",
    "    predictions_test.append(l)\n",
    "\n",
    "# calculate accuracy\n",
    "correct = 0\n",
    "for l,p in zip(sentiments_test,predictions_test):\n",
    "    if l == p:\n",
    "        correct = correct + 1\n",
    "print('Accuracy of the model = {}'.format(correct/len(sentiments_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2** Implement a neural network for sentiment classification. \n",
    "\n",
    "> ### System Configuration\n",
    "This part requires you to use a computer with `tensorflow` library installed. More information is available here - https://www.tensorflow.org.\n",
    "`\n",
    "You are allowed to implement the project on your personal computers using `Python 3.4 or above. You will need `numpy` and `scipy` libraries. If you need to use departmental resources, you can use **metallica.cse.buffalo.edu**, which has `Python 3.4.3` and the required libraries installed. \n",
    "\n",
    "> Students attempting to use the `tensorflow` library have two options: \n",
    "1. Install `tensorflow` on personal machines. Detailed installation information is here - https://www.tensorflow.org/. Note that, since `tensorflow` is a relatively new library, you might encounter installation issues depending on your OS and other library versions. We will not be providing any detailed support regarding `tensorflow` installation. If issues persist, we recommend using option 2. \n",
    "2. Use **metallica.cse.buffalo.edu**. If you are registered into the class, you should have an account on that server. The server already has Python 3.4.3 and TensorFlow 0.12.1 installed. Please use /util/bin/python for Python 3. \n",
    "3. To maintain a ssh connection for a long-running task on a remote machine, use tools like `screen`. For more information: https://linuxize.com/post/how-to-use-linux-screen/ \n",
    "4. For running jupyter-notebook over a remote machine find information on: https://fizzylogic.nl/2017/11/06/edit-jupyter-notebooks-over-ssh/\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def create_input_vector(review,word2index):\n",
    "    '''\n",
    "    Function to count how many times each word is used in the given review,\n",
    "    # and then store those counts at the appropriate indices inside x.\n",
    "    '''\n",
    "    vocab_size = len(word2index)\n",
    "    x = np.zeros((1, vocab_size))\n",
    "    for w in review.split(' '):\n",
    "        if w in word2index.keys():\n",
    "            x[0][word2index[w]] += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def find_ignore_words(pos_neg_ratios):\n",
    "    '''\n",
    "    Function to identify words to ignore from the vocabulary\n",
    "    '''\n",
    "    ignore_words = []\n",
    "    # TODO: Complete the implementation of find_ignore_words\n",
    "    \n",
    "    for i in pos_neg_ratios:\n",
    "        if pos_neg_ratios[i] == 0:\n",
    "            ignore_words.append(i)    \n",
    "    \n",
    "    return ignore_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# create a word2index mapping from word to an integer index\n",
    "word2index = {}\n",
    "ignore_words = find_ignore_words(pos_neg_ratios)\n",
    "vocab_selected = list(set(vocab_selected).difference(set(ignore_words)))\n",
    "for i,word in enumerate(vocab_selected):\n",
    "    if word not in ignore_words:\n",
    "        word2index[word] = i\n",
    "vocab_size = len(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate .hdf5 files from the processed data\n",
    "Given that the data is moderately large sized, the `hdf5` file format provides a more efficient file representation for further processing. See [here](https://anaconda.org/anaconda/hdf5) for more details and installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Run the script once to generate the file \n",
    "# delete the exiting 'data1.hdf5' file before running it again to avoid error \n",
    "labels_train = np.zeros((len(sentiments_train), 2), dtype=int)\n",
    "labels_test = np.zeros((len(sentiments_test), 2), dtype=int)\n",
    "\n",
    "with h5py.File('data1.hdf5', 'w') as hf:\n",
    "    hf.create_dataset('data_train', (labels_train.shape[0], vocab_size), np.int16)\n",
    "    hf.create_dataset('data_test', (labels_test.shape[0], vocab_size), np.int16)\n",
    "    # create training data\n",
    "    for i,(r,l) in enumerate(zip(reviews_train, sentiments_train)):\n",
    "        hf[\"data_train\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_train[i, 0] = 1\n",
    "        else:\n",
    "            labels_train[i, 1] = 1\n",
    "    # create test data\n",
    "    for i,(r,l) in enumerate(zip(reviews_test, sentiments_test)):\n",
    "        hf[\"data_test\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_test[i, 0] = 1\n",
    "        else:\n",
    "            labels_test[i, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.compat.v1.random.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the network\n",
    "learning_rate = 0.01\n",
    "batch_size = 400\n",
    "num_epochs = 50\n",
    "n_input = vocab_size\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 10  # 1st layer number of neurons\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_1, weights['out1']) + biases['out2'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X)\n",
    "# Define loss(softmax_cross_entropy_with_logits) and optimizer(AdamOptimizer)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the following two lines to resolve that issue\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.507292, Test_acc: 0.501250\n",
      "Train acc: 0.504542, Test_acc: 0.506250\n",
      "Train acc: 0.509458, Test_acc: 0.512500\n",
      "Train acc: 0.642833, Test_acc: 0.673750\n",
      "Train acc: 0.770833, Test_acc: 0.770000\n",
      "Train acc: 0.813250, Test_acc: 0.795000\n",
      "Train acc: 0.833542, Test_acc: 0.820000\n",
      "Train acc: 0.849583, Test_acc: 0.812500\n",
      "Train acc: 0.861708, Test_acc: 0.816250\n",
      "Train acc: 0.857292, Test_acc: 0.838750\n",
      "Train acc: 0.880458, Test_acc: 0.838750\n",
      "Train acc: 0.890417, Test_acc: 0.835000\n",
      "Train acc: 0.896750, Test_acc: 0.831250\n",
      "Train acc: 0.905833, Test_acc: 0.843750\n",
      "Train acc: 0.905417, Test_acc: 0.837500\n",
      "Train acc: 0.915000, Test_acc: 0.842500\n",
      "Train acc: 0.915417, Test_acc: 0.832500\n",
      "Train acc: 0.907375, Test_acc: 0.842500\n",
      "Train acc: 0.916667, Test_acc: 0.843750\n",
      "Train acc: 0.923458, Test_acc: 0.842500\n",
      "Train acc: 0.925000, Test_acc: 0.841250\n",
      "Train acc: 0.927417, Test_acc: 0.850000\n",
      "Train acc: 0.927958, Test_acc: 0.842500\n",
      "Train acc: 0.928375, Test_acc: 0.845000\n",
      "Train acc: 0.932500, Test_acc: 0.851250\n",
      "Train acc: 0.935208, Test_acc: 0.850000\n",
      "Train acc: 0.935000, Test_acc: 0.845000\n",
      "Train acc: 0.934000, Test_acc: 0.847500\n",
      "Train acc: 0.927500, Test_acc: 0.845000\n",
      "Train acc: 0.927000, Test_acc: 0.847500\n",
      "Train acc: 0.920625, Test_acc: 0.848750\n",
      "Train acc: 0.929958, Test_acc: 0.858750\n",
      "Train acc: 0.931375, Test_acc: 0.850000\n",
      "Train acc: 0.934917, Test_acc: 0.855000\n",
      "Train acc: 0.938583, Test_acc: 0.853750\n",
      "Train acc: 0.941250, Test_acc: 0.852500\n",
      "Train acc: 0.942042, Test_acc: 0.847500\n",
      "Train acc: 0.943375, Test_acc: 0.850000\n",
      "Train acc: 0.944375, Test_acc: 0.855000\n",
      "Train acc: 0.943667, Test_acc: 0.853750\n",
      "Train acc: 0.942250, Test_acc: 0.857500\n",
      "Train acc: 0.940500, Test_acc: 0.856250\n",
      "Train acc: 0.941042, Test_acc: 0.853750\n",
      "Train acc: 0.944750, Test_acc: 0.862500\n",
      "Train acc: 0.944875, Test_acc: 0.862500\n",
      "Train acc: 0.945125, Test_acc: 0.862500\n",
      "Train acc: 0.945958, Test_acc: 0.860000\n",
      "Train acc: 0.945083, Test_acc: 0.850000\n",
      "Train acc: 0.943833, Test_acc: 0.852500\n",
      "Train acc: 0.942833, Test_acc: 0.852500\n",
      "Time elapsed - 22.68907117843628 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    h = h5py.File('data1.hdf5', 'r')\n",
    "    n1 = h.get('data_train') \n",
    "    n2 = h.get('data_test')\n",
    "\n",
    "    # Training cycle\n",
    "    total_batch_train = int(n1.shape[0] / batch_size)\n",
    "    total_batch_test = int(n2.shape[0] / batch_size)\n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        # variables for train and test accuracies\n",
    "        avg_acc_train = 0.\n",
    "        avg_acc_test = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
    "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
    "\n",
    "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
    "            _label_train = [np.argmax(i) for i in _logits_train]\n",
    "            _label_train_y = [np.argmax(i) for i in train_y]\n",
    "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
    "            avg_acc_train += _accuracy_train\n",
    "\n",
    "\n",
    "        for j in range(total_batch_test):\n",
    "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
    "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
    "\n",
    "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
    "            _label_test = [np.argmax(i) for i in _logits_test]\n",
    "            _label_test_y = [np.argmax(i) for i in test_y]\n",
    "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
    "            avg_acc_test += _accuracy_test\n",
    "\n",
    "        # print the train and test accuracies   \n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
    "    duration = time.time() - start_time\n",
    "    print('Time elapsed - {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Added another hidden layer with width of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# parameters of the network\n",
    "learning_rate = 0.01\n",
    "batch_size = 400\n",
    "num_epochs = 50 #### Increased Epochs from 50 to 100 ###\n",
    "n_input = vocab_size\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 32  # 1st layer number of neurons\n",
    "n_hidden_2 = 32 ######## ADDED SECOND LAYER ############\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), #### combined  ####\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])), #### New Bias ####\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1) #### activation function --> Makes NN work better ####\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) #### Added second layer ####\n",
    "    layer_2 = tf.nn.relu(layer_2) #### activation function --> Makes NN work better ####\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_2, weights['out1']) + biases['out2'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X)\n",
    "# Define loss(softmax_cross_entropy_with_logits) and optimizer(AdamOptimizer)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) ### Changing to different optimizer might be better ###\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# for some macosx installations, conflicting copies of mpilib causes trouble with tensorflow.\n",
    "# use the following two lines to resolve that issue\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.517875, Test_acc: 0.571250\n",
      "Train acc: 0.527958, Test_acc: 0.500000\n",
      "Train acc: 0.501583, Test_acc: 0.505000\n",
      "Train acc: 0.589583, Test_acc: 0.646250\n",
      "Train acc: 0.643625, Test_acc: 0.605000\n",
      "Train acc: 0.703417, Test_acc: 0.662500\n",
      "Train acc: 0.673333, Test_acc: 0.583750\n",
      "Train acc: 0.724792, Test_acc: 0.738750\n",
      "Train acc: 0.724958, Test_acc: 0.740000\n",
      "Train acc: 0.766750, Test_acc: 0.677500\n",
      "Train acc: 0.711958, Test_acc: 0.655000\n",
      "Train acc: 0.708458, Test_acc: 0.731250\n",
      "Train acc: 0.735500, Test_acc: 0.641250\n",
      "Train acc: 0.768458, Test_acc: 0.777500\n",
      "Train acc: 0.802417, Test_acc: 0.772500\n",
      "Train acc: 0.828000, Test_acc: 0.796250\n",
      "Train acc: 0.832042, Test_acc: 0.796250\n",
      "Train acc: 0.813250, Test_acc: 0.807500\n",
      "Train acc: 0.826167, Test_acc: 0.823750\n",
      "Train acc: 0.806208, Test_acc: 0.791250\n",
      "Train acc: 0.840208, Test_acc: 0.792500\n",
      "Train acc: 0.832708, Test_acc: 0.823750\n",
      "Train acc: 0.858292, Test_acc: 0.827500\n",
      "Train acc: 0.856667, Test_acc: 0.815000\n",
      "Train acc: 0.852375, Test_acc: 0.826250\n",
      "Train acc: 0.851458, Test_acc: 0.765000\n",
      "Train acc: 0.832083, Test_acc: 0.820000\n",
      "Train acc: 0.866000, Test_acc: 0.817500\n",
      "Train acc: 0.845250, Test_acc: 0.758750\n",
      "Train acc: 0.849708, Test_acc: 0.831250\n",
      "Train acc: 0.864083, Test_acc: 0.833750\n",
      "Train acc: 0.862583, Test_acc: 0.791250\n",
      "Train acc: 0.844375, Test_acc: 0.837500\n",
      "Train acc: 0.866792, Test_acc: 0.820000\n",
      "Train acc: 0.857875, Test_acc: 0.793750\n",
      "Train acc: 0.860292, Test_acc: 0.828750\n",
      "Train acc: 0.865292, Test_acc: 0.805000\n",
      "Train acc: 0.869083, Test_acc: 0.830000\n",
      "Train acc: 0.877375, Test_acc: 0.815000\n",
      "Train acc: 0.858708, Test_acc: 0.833750\n",
      "Train acc: 0.878667, Test_acc: 0.831250\n",
      "Train acc: 0.878708, Test_acc: 0.826250\n",
      "Train acc: 0.879250, Test_acc: 0.827500\n",
      "Train acc: 0.880250, Test_acc: 0.842500\n",
      "Train acc: 0.881417, Test_acc: 0.823750\n",
      "Train acc: 0.867167, Test_acc: 0.821250\n",
      "Train acc: 0.873500, Test_acc: 0.831250\n",
      "Train acc: 0.887125, Test_acc: 0.833750\n",
      "Train acc: 0.884000, Test_acc: 0.837500\n",
      "Train acc: 0.880042, Test_acc: 0.826250\n",
      "Time elapsed - 36.45852184295654 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    h = h5py.File('data1.hdf5', 'r')\n",
    "    n1 = h.get('data_train') \n",
    "    n2 = h.get('data_test')\n",
    "\n",
    "    # Training cycle\n",
    "    total_batch_train = int(n1.shape[0] / batch_size)\n",
    "    total_batch_test = int(n2.shape[0] / batch_size)\n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        # variables for train and test accuracies\n",
    "        avg_acc_train = 0.\n",
    "        avg_acc_test = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
    "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
    "\n",
    "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
    "            _label_train = [np.argmax(i) for i in _logits_train]\n",
    "            _label_train_y = [np.argmax(i) for i in train_y]\n",
    "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
    "            avg_acc_train += _accuracy_train\n",
    "\n",
    "\n",
    "        for j in range(total_batch_test):\n",
    "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
    "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
    "\n",
    "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
    "            _label_test = [np.argmax(i) for i in _logits_test]\n",
    "            _label_test_y = [np.argmax(i) for i in test_y]\n",
    "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
    "            avg_acc_test += _accuracy_test\n",
    "\n",
    "        # print the train and test accuracies   \n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
    "    duration = time.time() - start_time\n",
    "    print('Time elapsed - {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Epich to be 100 and single hidden layer width to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the network\n",
    "learning_rate = 0.01\n",
    "batch_size = 400\n",
    "num_epochs = 100 #### Increased Epochs from 50 to 100 ###\n",
    "n_input = vocab_size\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 64  # 1st layer number of neurons\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_1, weights['out1']) + biases['out2'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X)\n",
    "# Define loss(softmax_cross_entropy_with_logits) and optimizer(AdamOptimizer)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.551667, Test_acc: 0.601250\n",
      "Train acc: 0.658500, Test_acc: 0.655000\n",
      "Train acc: 0.717125, Test_acc: 0.705000\n",
      "Train acc: 0.773042, Test_acc: 0.767500\n",
      "Train acc: 0.798917, Test_acc: 0.766250\n",
      "Train acc: 0.821958, Test_acc: 0.791250\n",
      "Train acc: 0.825875, Test_acc: 0.810000\n",
      "Train acc: 0.822500, Test_acc: 0.813750\n",
      "Train acc: 0.855458, Test_acc: 0.792500\n",
      "Train acc: 0.852292, Test_acc: 0.827500\n",
      "Train acc: 0.857792, Test_acc: 0.831250\n",
      "Train acc: 0.870125, Test_acc: 0.825000\n",
      "Train acc: 0.866500, Test_acc: 0.816250\n",
      "Train acc: 0.864000, Test_acc: 0.818750\n",
      "Train acc: 0.871750, Test_acc: 0.828750\n",
      "Train acc: 0.876417, Test_acc: 0.828750\n",
      "Train acc: 0.880792, Test_acc: 0.812500\n",
      "Train acc: 0.884417, Test_acc: 0.821250\n",
      "Train acc: 0.882583, Test_acc: 0.832500\n",
      "Train acc: 0.877167, Test_acc: 0.833750\n",
      "Train acc: 0.891583, Test_acc: 0.837500\n",
      "Train acc: 0.891375, Test_acc: 0.828750\n",
      "Train acc: 0.894333, Test_acc: 0.830000\n",
      "Train acc: 0.884625, Test_acc: 0.830000\n",
      "Train acc: 0.897292, Test_acc: 0.822500\n",
      "Train acc: 0.885583, Test_acc: 0.837500\n",
      "Train acc: 0.898750, Test_acc: 0.832500\n",
      "Train acc: 0.897167, Test_acc: 0.830000\n",
      "Train acc: 0.894750, Test_acc: 0.828750\n",
      "Train acc: 0.890458, Test_acc: 0.828750\n",
      "Train acc: 0.888667, Test_acc: 0.820000\n",
      "Train acc: 0.896750, Test_acc: 0.830000\n",
      "Train acc: 0.896625, Test_acc: 0.833750\n",
      "Train acc: 0.902292, Test_acc: 0.843750\n",
      "Train acc: 0.898625, Test_acc: 0.841250\n",
      "Train acc: 0.905500, Test_acc: 0.841250\n",
      "Train acc: 0.904875, Test_acc: 0.837500\n",
      "Train acc: 0.903958, Test_acc: 0.835000\n",
      "Train acc: 0.907417, Test_acc: 0.836250\n",
      "Train acc: 0.902125, Test_acc: 0.836250\n",
      "Train acc: 0.907917, Test_acc: 0.836250\n",
      "Train acc: 0.909917, Test_acc: 0.835000\n",
      "Train acc: 0.903125, Test_acc: 0.838750\n",
      "Train acc: 0.910458, Test_acc: 0.851250\n",
      "Train acc: 0.902917, Test_acc: 0.855000\n",
      "Train acc: 0.909292, Test_acc: 0.857500\n",
      "Train acc: 0.899833, Test_acc: 0.845000\n",
      "Train acc: 0.911167, Test_acc: 0.842500\n",
      "Train acc: 0.912750, Test_acc: 0.847500\n",
      "Train acc: 0.907625, Test_acc: 0.848750\n",
      "Train acc: 0.913750, Test_acc: 0.846250\n",
      "Train acc: 0.911542, Test_acc: 0.851250\n",
      "Train acc: 0.914542, Test_acc: 0.848750\n",
      "Train acc: 0.912250, Test_acc: 0.850000\n",
      "Train acc: 0.909250, Test_acc: 0.840000\n",
      "Train acc: 0.902583, Test_acc: 0.848750\n",
      "Train acc: 0.910292, Test_acc: 0.846250\n",
      "Train acc: 0.915417, Test_acc: 0.845000\n",
      "Train acc: 0.917083, Test_acc: 0.837500\n",
      "Train acc: 0.913875, Test_acc: 0.843750\n",
      "Train acc: 0.915167, Test_acc: 0.843750\n",
      "Train acc: 0.914333, Test_acc: 0.837500\n",
      "Train acc: 0.916583, Test_acc: 0.842500\n",
      "Train acc: 0.918875, Test_acc: 0.841250\n",
      "Train acc: 0.919125, Test_acc: 0.846250\n",
      "Train acc: 0.919667, Test_acc: 0.848750\n",
      "Train acc: 0.920375, Test_acc: 0.848750\n",
      "Train acc: 0.920542, Test_acc: 0.853750\n",
      "Train acc: 0.920958, Test_acc: 0.836250\n",
      "Train acc: 0.920750, Test_acc: 0.858750\n",
      "Train acc: 0.920875, Test_acc: 0.858750\n",
      "Train acc: 0.917500, Test_acc: 0.848750\n",
      "Train acc: 0.919125, Test_acc: 0.847500\n",
      "Train acc: 0.921333, Test_acc: 0.852500\n",
      "Train acc: 0.920667, Test_acc: 0.847500\n",
      "Train acc: 0.922458, Test_acc: 0.851250\n",
      "Train acc: 0.923292, Test_acc: 0.847500\n",
      "Train acc: 0.919000, Test_acc: 0.852500\n",
      "Train acc: 0.919792, Test_acc: 0.852500\n",
      "Train acc: 0.914500, Test_acc: 0.842500\n",
      "Train acc: 0.921208, Test_acc: 0.836250\n",
      "Train acc: 0.921167, Test_acc: 0.843750\n",
      "Train acc: 0.923042, Test_acc: 0.842500\n",
      "Train acc: 0.920667, Test_acc: 0.840000\n",
      "Train acc: 0.914750, Test_acc: 0.845000\n",
      "Train acc: 0.921417, Test_acc: 0.850000\n",
      "Train acc: 0.923542, Test_acc: 0.843750\n",
      "Train acc: 0.923000, Test_acc: 0.843750\n",
      "Train acc: 0.917417, Test_acc: 0.835000\n",
      "Train acc: 0.922333, Test_acc: 0.851250\n",
      "Train acc: 0.917083, Test_acc: 0.850000\n",
      "Train acc: 0.925542, Test_acc: 0.856250\n",
      "Train acc: 0.921750, Test_acc: 0.843750\n",
      "Train acc: 0.921542, Test_acc: 0.848750\n",
      "Train acc: 0.925708, Test_acc: 0.851250\n",
      "Train acc: 0.923833, Test_acc: 0.857500\n",
      "Train acc: 0.926708, Test_acc: 0.855000\n",
      "Train acc: 0.924625, Test_acc: 0.856250\n",
      "Train acc: 0.925792, Test_acc: 0.852500\n",
      "Train acc: 0.925833, Test_acc: 0.853750\n",
      "Time elapsed - 42.970091104507446 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    h = h5py.File('data1.hdf5', 'r')\n",
    "    n1 = h.get('data_train') \n",
    "    n2 = h.get('data_test')\n",
    "\n",
    "    # Training cycle\n",
    "    total_batch_train = int(n1.shape[0] / batch_size)\n",
    "    total_batch_test = int(n2.shape[0] / batch_size)\n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        # variables for train and test accuracies\n",
    "        avg_acc_train = 0.\n",
    "        avg_acc_test = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
    "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
    "\n",
    "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
    "            _label_train = [np.argmax(i) for i in _logits_train]\n",
    "            _label_train_y = [np.argmax(i) for i in train_y]\n",
    "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
    "            avg_acc_train += _accuracy_train\n",
    "\n",
    "\n",
    "        for j in range(total_batch_test):\n",
    "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
    "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
    "\n",
    "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
    "            _label_test = [np.argmax(i) for i in _logits_test]\n",
    "            _label_test_y = [np.argmax(i) for i in test_y]\n",
    "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
    "            avg_acc_test += _accuracy_test\n",
    "\n",
    "        # print the train and test accuracies   \n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
    "    duration = time.time() - start_time\n",
    "    print('Time elapsed - {} seconds.'.format(duration))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two hidden layers of 32 width, 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the network\n",
    "learning_rate = 0.01\n",
    "batch_size = 400\n",
    "num_epochs = 100 #### Increased Epochs from 50 to 100 ###\n",
    "n_input = vocab_size\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 64  # 1st layer number of neurons\n",
    "n_hidden_2 = 64 ######## ADDED SECOND LAYER ############\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), #### combined  ####\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])), #### New Bias ####\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1) #### activation function --> Makes NN work better ####\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) #### Added second layer ####\n",
    "    layer_2 = tf.nn.relu(layer_2) #### activation function --> Makes NN work better ####\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_2, weights['out1']) + biases['out2'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X)\n",
    "# Define loss(softmax_cross_entropy_with_logits) and optimizer(AdamOptimizer)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) ### Changing to different optimizer might be better ###\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some macosx installations, conflicting copies of mpilib causes trouble with tensorflow.\n",
    "# use the following two lines to resolve that issue\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.499125, Test_acc: 0.501250\n",
      "Train acc: 0.500458, Test_acc: 0.500000\n",
      "Train acc: 0.500542, Test_acc: 0.501250\n",
      "Train acc: 0.500583, Test_acc: 0.502500\n",
      "Train acc: 0.500667, Test_acc: 0.502500\n",
      "Train acc: 0.500625, Test_acc: 0.502500\n",
      "Train acc: 0.500625, Test_acc: 0.502500\n",
      "Train acc: 0.500583, Test_acc: 0.502500\n",
      "Train acc: 0.500583, Test_acc: 0.502500\n",
      "Train acc: 0.500625, Test_acc: 0.502500\n",
      "Train acc: 0.500625, Test_acc: 0.502500\n",
      "Train acc: 0.500667, Test_acc: 0.502500\n",
      "Train acc: 0.500583, Test_acc: 0.500000\n",
      "Train acc: 0.511667, Test_acc: 0.532500\n",
      "Train acc: 0.505542, Test_acc: 0.500000\n",
      "Train acc: 0.501208, Test_acc: 0.500000\n",
      "Train acc: 0.502208, Test_acc: 0.501250\n",
      "Train acc: 0.506083, Test_acc: 0.500000\n",
      "Train acc: 0.526042, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Train acc: 0.500000, Test_acc: 0.500000\n",
      "Time elapsed - 49.04177498817444 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    h = h5py.File('data1.hdf5', 'r')\n",
    "    n1 = h.get('data_train') \n",
    "    n2 = h.get('data_test')\n",
    "\n",
    "    # Training cycle\n",
    "    total_batch_train = int(n1.shape[0] / batch_size)\n",
    "    total_batch_test = int(n2.shape[0] / batch_size)\n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        # variables for train and test accuracies\n",
    "        avg_acc_train = 0.\n",
    "        avg_acc_test = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
    "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
    "\n",
    "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
    "            _label_train = [np.argmax(i) for i in _logits_train]\n",
    "            _label_train_y = [np.argmax(i) for i in train_y]\n",
    "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
    "            avg_acc_train += _accuracy_train\n",
    "\n",
    "\n",
    "        for j in range(total_batch_test):\n",
    "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
    "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
    "\n",
    "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
    "            _label_test = [np.argmax(i) for i in _logits_test]\n",
    "            _label_test_y = [np.argmax(i) for i in test_y]\n",
    "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
    "            avg_acc_test += _accuracy_test\n",
    "\n",
    "        # print the train and test accuracies   \n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
    "    duration = time.time() - start_time\n",
    "    print('Time elapsed - {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
