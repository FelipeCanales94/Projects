{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyglet==1.4.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atari DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from collections import deque\n",
    "from keras import backend as back\n",
    "back.image_data_format()\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, merge, Lambda, Input, Add, Conv2D, LeakyReLU, Flatten, Multiply, Reshape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to break down images shot by shot\n",
    "def processImage(frame):\n",
    "    frame = np.dot(frame[..., :3], [0.299, 0.587, 0.114])\n",
    "    frame = cv.resize(frame, dsize=(84, 110), interpolation=cv.INTER_AREA)\n",
    "    frame = frame[16:100]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariDQN:\n",
    "    cp_env = gym.make('Breakout-v0')\n",
    "    def __init__(self, sizeOfState, sizeOfAction):\n",
    "        \n",
    "        # best parameters I can come up with\n",
    "        self.sizeOfState = (84, 84, 4)\n",
    "        self.sizeOfAction = sizeOfAction\n",
    "\n",
    "        self.gamma = .99\n",
    "        self.minEpsilon = .1\n",
    "        self.maxEpsilon = 1.0\n",
    "        self.decay = .999\n",
    "        self.batchSize = 32\n",
    "        self.training = 50000\n",
    "        self.mem = deque(maxlen=1000000)\n",
    "        self.saveImage = deque(maxlen=4)\n",
    "        \n",
    "        \n",
    "        self.model = self.ddqnModel()\n",
    "        self.target = self.ddqnModel()\n",
    "        \n",
    "    def ddqnModel(self):\n",
    "        \n",
    "        \n",
    "        # create, seperate then aggregate layers for qvalue\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=self.sizeOfState))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.sizeOfAction))\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, epsilon=0.01))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    \n",
    "    # update target model\n",
    "    def updateModel(self):\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "        return\n",
    "    \n",
    "    # get action - either explore or exploit\n",
    "    def actions(self, s):\n",
    "        s = np.reshape(s, (-1, 84, 84, 4))\n",
    "        # Exploration vs Exploitation\n",
    "        if np.random.random()<= self.maxEpsilon :\n",
    "            return random.randrange(self.sizeOfAction)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(s))\n",
    "        \n",
    "    # store experience in memory\n",
    "    def memReplay(self, s, a, r, ns, done):\n",
    "        s = np.reshape(s, (-1, 84, 84, 4))\n",
    "        ns = np.reshape(ns, (-1, 84, 84, 4))\n",
    "        self.mem.append((s, a, r, ns, done))\n",
    "        \n",
    "        if len(self.mem) > self.training:\n",
    "            if self.maxEpsilon > self.minEpsilon:\n",
    "                self.maxEpsilon *= self.decay\n",
    "\n",
    "                \n",
    "    # use that experience to train\n",
    "    def repSample(self):\n",
    "        \n",
    "        # get random sample for manibatch\n",
    "        miniBatch = random.sample(self.mem, self.batchSize)\n",
    "\n",
    "        current = np.zeros((self.batchSize, self.sizeOfState[0],self.sizeOfState[1],self.sizeOfState[2]))\n",
    "        ns = np.zeros((self.batchSize, self.sizeOfState[0],self.sizeOfState[1],self.sizeOfState[2]))\n",
    "        a, r, done = [], [], []\n",
    "\n",
    "        for x in range(self.batchSize):\n",
    "            current[x] = miniBatch[x][0]\n",
    "            a.append(miniBatch[x][1])\n",
    "            r.append(miniBatch[x][2])\n",
    "            ns[x] = miniBatch[x][3]\n",
    "            done.append(miniBatch[x][4])\n",
    "        \n",
    "        # get target and next target\n",
    "        #print(current.shape)\n",
    "        tar = self.model.predict(current)\n",
    "        nt = self.model.predict(ns)\n",
    "\n",
    "        for x in range(len(miniBatch)):\n",
    "            if done[x]:\n",
    "                tar[x][a[x]] = r[x]\n",
    "            else:\n",
    "                # formula\n",
    "                tar[x][a[x]] = r[x] + self.gamma * (np.amax(nt[x]))\n",
    "\n",
    "        self.model.fit(current, tar, batch_size=self.batchSize, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_env = gym.make('Breakout-v0')\n",
    "sizeOfState = (84, 84, 4)\n",
    "sizeOfActions = bo_env.action_space.n\n",
    "\n",
    "obs = processImage(bo_env.reset())\n",
    "\n",
    "scores, episodes, eps = [],[],[] # for stats\n",
    "\n",
    "agent = AtariDQN(sizeOfState, sizeOfActions) # initialize agent\n",
    "\n",
    "for episode in range(10000):\n",
    "            \n",
    "    current = processImage(bo_env.reset()) # fresh start after every episode\n",
    "    agent.saveImage.extend([current] * 4)\n",
    "\n",
    "    #current = np.reshape(current, [1, sizeOfState[0],sizeOfState[1],sizeOfState[2]])\n",
    "    done = False\n",
    "    i = 0 # score tracker\n",
    "    s = 0\n",
    "\n",
    "    # keep going until done\n",
    "    while not done:\n",
    "\n",
    "        processed = processImage(current)\n",
    "        agent.saveImage.append(processed)\n",
    "        a = 0\n",
    "\n",
    "        # Do nothing first action to avoid being in sub-optimal policy\n",
    "        if len(agent.saveImage) < 4:\n",
    "            ns, r, done, info = bo_env.step(a)\n",
    "        else:\n",
    "            # save everything then store in memory\n",
    "            now = np.stack([agent.saveImage[0], agent.saveImage[1], agent.saveImage[2], agent.saveImage[3]])\n",
    "            a = agent.actions(now)\n",
    "            ns, r, done, info = bo_env.step(a)\n",
    "            s += r\n",
    "            r = r if not done else -10\n",
    "            nsImage = processImage(ns)\n",
    "            ns = np.stack([agent.saveImage[0], agent.saveImage[1], agent.saveImage[2], nsImage])\n",
    "\n",
    "            agent.memReplay(now, a, r, ns, done) # store in memory\n",
    "\n",
    "        current = ns\n",
    "\n",
    "        if done:\n",
    "            # update model\n",
    "            agent.updateModel()\n",
    "\n",
    "            # stats\n",
    "            score = int(s)\n",
    "            scores.append(score)\n",
    "            episodes.append(episode)\n",
    "            eps.append(agent.maxEpsilon)\n",
    "            print('Episode: ', episode, ' Score: ', score, 'Epsilon: ', agent.maxEpsilon)\n",
    "            if i == bo_env._max_episode_steps:\n",
    "                break\n",
    "\n",
    "# plotting\n",
    "plt.plot(episodes, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
