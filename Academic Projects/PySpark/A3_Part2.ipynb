{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace, monotonically_increasing_id, split\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover, HashingTF, IDF, OneHotEncoderEstimator, StringIndexer, VectorAssembler, Binarizer\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: long (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- plot: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[movie_id: bigint, movie_name: string, plot: string, genre: string]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new spark session, getting data, creating spark dataframe\n",
    "\n",
    "\n",
    "# You may be able to figure something out better\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "        .config(\"spark.executor.memory\", \"32g\")\\\n",
    "        .config(\"spark.driver.memory\", \"32g\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"32g\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark2 = SparkSession.builder.master(\"local[*]\")\\\n",
    "        .config(\"spark.executor.memory\", \"32g\")\\\n",
    "        .config(\"spark.driver.memory\", \"32g\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"32g\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "x = pd.read_csv('train.csv')\n",
    "df = spark.createDataFrame(x)\n",
    "\n",
    "df.printSchema()\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------+\n",
      "|                plot|               genre|               words|            filtered|            features| label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------+\n",
      "|Shlykov, a hard-w...|['World cinema', ...|[shlykov, a, hard...|[shlykov, a, hard...|(10000,[1,2,18,26...|   4.0|\n",
      "|The nation of Pan...|['Action/Adventur...|[the, nation, of,...|[nation, of, pane...|(10000,[0,1,2,3,4...|1603.0|\n",
      "|Poovalli Induchoo...|['Musical', 'Acti...|[poovalli, induch...|[poovalli, induch...|(10000,[0,1,2,3,4...| 316.0|\n",
      "|The Lemon Drop Ki...|          ['Comedy']|[the, lemon, drop...|[lemon, drop, kid...|(10000,[0,1,2,3,4...|   1.0|\n",
      "|Seventh-day Adven...|['Crime Fiction',...|[seventh, day, ad...|[seventh, day, ad...|(10000,[0,1,2,3,4...| 113.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#Drop irrelevant features\n",
    "drop_list = ['movie_id', 'movie_name']\n",
    "data = df.select([column for column in df.columns if column not in drop_list])\n",
    "\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] # standard stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "label_stringIdx = StringIndexer(inputCol = \"genre\", outputCol = \"label\")\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier TF-IDF accuracy:  0.11836558209923252\n",
      "RandomForestClassifier TF-IDF precision:  0.7363819771351715\n",
      "RandomForestClassifier TF-IDF recall:  1.0\n",
      "RandomForestClassifier TF-IDF f1score:  0.8481797056545314\n"
     ]
    }
   ],
   "source": [
    "#RANDOM FOREST using TF-IDF Features\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "\n",
    "#dataset = pipelineFit.transform(dataset)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "# Train model with Training Data\n",
    "rf_tfidf_Model = rf.fit(trainingData)\n",
    "predictions = rf_tfidf_Model.transform(testData)\n",
    "results = predictions.select(['prediction', 'label'])\n",
    "predictionAndLabels=results.rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "cm = metrics.confusionMatrix().toArray()\n",
    "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "precision=(cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "recall=(cm[0][0])/(cm[0][0]+cm[0][1])\n",
    "f1score = 2*(precision*recall)/(precision+recall)\n",
    "print(\"RandomForestClassifier TF-IDF accuracy: \",accuracy)\n",
    "print(\"RandomForestClassifier TF-IDF precision: \",precision)\n",
    "print(\"RandomForestClassifier TF-IDF recall: \", recall)\n",
    "print(\"RandomForestClassifier TF-IDF f1score: \", f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new spark session, getting data, creating spark dataframe\n",
    "\n",
    "# You may be able to figure something out better\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "        .config(\"spark.executor.memory\", \"32g\")\\\n",
    "        .config(\"spark.driver.memory\", \"32g\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"32g\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark2 = SparkSession.builder.master(\"local[*]\")\\\n",
    "        .config(\"spark.executor.memory\", \"32g\")\\\n",
    "        .config(\"spark.driver.memory\", \"32g\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"32g\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "x = pd.read_csv('test.csv')\n",
    "df = spark2.createDataFrame(x)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#Drop irrelevant features\n",
    "drop_list = ['movie_id', 'movie_name']\n",
    "data = df.select([column for column in df.columns if column not in drop_list])\n",
    "\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] # standard stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "label_stringIdx = StringIndexer(inputCol = \"genre\", outputCol = \"label\")\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "\n",
    "rf_predictions = rf_tfidf_Model.transform(dataset)\n",
    "results = predictions.select(['prediction', 'label'])\n",
    "predictionAndLabels=results.rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "cm = metrics.confusionMatrix().toArray()\n",
    "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "precision=(cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "recall=(cm[0][0])/(cm[0][0]+cm[0][1])\n",
    "f1score = 2*(precision*recall)/(precision+recall)\n",
    "print(\"RandomForestClassifier: accuracy, precision, recall, f1score\",accuracy,precision,recall,f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "f = IndexToString(\n",
    "    inputCol=\"prediction\", outputCol=\"categoryValue\")\n",
    "\n",
    "f.transform(results).drop(\"id\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
